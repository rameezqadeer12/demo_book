# -*- coding: utf-8 -*-
"""hybridsystem .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1683gJT7Je4klmOXKcS6vZ06FpRknL8C2
"""

from google.colab import drive
drive.mount("/content/drive")

import os

BASE = "/content/drive/MyDrive/real_rag_store/kaggle/working/punjab_exam_system/extracted_text_v2"

print("Exists:", os.path.exists(BASE))
print("Files/folders:", os.listdir(BASE))

import zipfile, os

ZIP_PATH = "/content/drive/MyDrive/books/chunks_fasis.zip"
OUT_DIR  = "/content/drive/MyDrive/final_chunks_faiss"

os.makedirs(OUT_DIR, exist_ok=True)

with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
    zip_ref.extractall(OUT_DIR)

print("‚úÖ Unzipped to:", OUT_DIR)
print("Files:", os.listdir(OUT_DIR))

!pip install -q huggingface-hub==0.34.1 transformers==4.41.2 sentence-transformers==2.7.0 faiss-cpu

!pip install -q llama-cpp-python==0.2.26

import os, pickle, faiss
from sentence_transformers import SentenceTransformer

STORE = "/content/drive/MyDrive/final_chunks_faiss/kaggle/working/punjab_exam_system"

CHUNKS_PATH = os.path.join(STORE, "chunks.pkl")
INDEX_PATH  = os.path.join(STORE, "index.faiss")

assert os.path.exists(CHUNKS_PATH), "‚ùå chunks.pkl not found"
assert os.path.exists(INDEX_PATH), "‚ùå index.faiss not found"

with open(CHUNKS_PATH, "rb") as f:
    chunks = pickle.load(f)

index = faiss.read_index(INDEX_PATH)

embedder = SentenceTransformer("all-MiniLM-L6-v2")

print("‚úÖ FINAL STORE LOADED")
print("Chunks:", len(chunks))
print("FAISS :", index.ntotal)

# =========================================
# FINAL PUNJAB EXAM RAG SYSTEM
# =========================================

import os, pickle, faiss, re
import numpy as np
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama


# ========= 1. PATHS (DO NOT CHANGE) =========

STORE = "/content/drive/MyDrive/final_chunks_faiss/kaggle/working/punjab_exam_system"
GGUF_PATH = "/content/drive/MyDrive/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"


# ========= 2. LOAD FAISS + CHUNKS =========

print("üîÑ Loading exam knowledge base...")

with open(os.path.join(STORE, "chunks.pkl"), "rb") as f:
    chunks = pickle.load(f)

index = faiss.read_index(os.path.join(STORE, "index.faiss"))
embedder = SentenceTransformer("all-MiniLM-L6-v2")

print("‚úÖ Knowledge base loaded")
print("   Chunks:", len(chunks))
print("   FAISS :", index.ntotal)


# ========= 3. LOAD LLaMA =========

print("üîÑ Loading LLaMA model...")

llm = Llama(
    model_path=GGUF_PATH,
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0
)

print("‚úÖ LLaMA loaded")


# ========= 4. RETRIEVER =========

def retrieve(query, top_k=5):
    q_emb = embedder.encode([query], normalize_embeddings=True)
    scores, ids = index.search(q_emb.astype("float32"), top_k)

    results = []
    for i, idx in enumerate(ids[0]):
        if idx == -1:
            continue

        r = chunks[idx]
        results.append({
            "text": r["text"],
            "source": r.get("source",""),
            "chunk_id": idx,
            "score": float(scores[0][i])
        })

    return results


# ========= 5. BOOK ANSWER BUILDER =========

def build_book_answer(retrieved, max_chars=600):
    parts, total = [], 0

    for r in retrieved:
        t = re.sub(r"\s+", " ", r["text"]).strip()

        if len(t) < 40:
            continue

        if total + len(t) > max_chars:
            t = t[:max_chars-total]

        parts.append(t)
        total += len(t)

        if total >= max_chars:
            break

    return " ".join(parts)


# ========= 6. PROMPT BUILDER =========

def build_prompt(book_answer, question):

    system = """You are a strict school teacher.
Only use the BOOK ANSWER.
If not found, say: NOT FOUND IN BOOK.
Explain simply like a teacher.
"""

    return f"""<s>[SYSTEM]
{system}
[/SYSTEM]
[USER]
BOOK ANSWER:
{book_answer}

QUESTION:
{question}

FORMAT:
üìò Book Answer:
üß† Teacher Explanation:
‚úèÔ∏è Solved Example:
‚úÖ Final Answer:
[/USER]
[ASSISTANT]
"""


# ========= 7. MAIN ASK FUNCTION =========

def ask_llama(question, top_k=5):
    retrieved = retrieve(question, top_k)

    if not retrieved:
        return "‚ùå NOT FOUND IN BOOK.", []

    book_answer = build_book_answer(retrieved)

    if len(book_answer) < 50:
        return "‚ùå NOT FOUND IN BOOK.", retrieved

    prompt = build_prompt(book_answer, question)

    out = llm(prompt, max_tokens=250, temperature=0.2)
    answer = out["choices"][0]["text"].strip()

    return answer, retrieved


# ========= 8. INTERACTIVE EXAM MODE =========

print("\nüéì PUNJAB EXAM AI READY")
print("Type a question.  Type 'exit' to stop.\n")

while True:
    q = input("‚ùì Question: ").strip()
    if q.lower() == "exit":
        break

    ans, src = ask_llama(q)

    print("\n=========== ANSWER ===========\n")
    print(ans)

    print("\n=========== SOURCES ==========\n")
    for s in src:
        print(s["source"], "| chunk", s["chunk_id"], "| score", round(s["score"],3))

    print("\n==============================\n")

!pip -q install -U "huggingface_hub>=0.23"